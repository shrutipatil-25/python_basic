{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGxBP-yQoCl"
      },
      "source": [
        "# Lesson 1: Using files in Python\n",
        "\n",
        "Hit the play button on the video next to this Jupyter notebook to start the video and follow along as Andrew explains how to work through this lesson.\n",
        "\n",
        "So far, if you have taken the previous courses in this sequence,\n",
        "* You have worked with data that is created and assigned to variables within Jupyter notebooks.\n",
        "* You have created multi-line strings.\n",
        "* You have created lists and dictionaries.\n",
        "* You have automated tasks using `for` loops and `if` statements.\n",
        "\n",
        "\n",
        "In this lesson, you will read files using Python!\n",
        "\n",
        "Let's start by loading some functions you'll use in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "id": "ykKhKum1ZqIV",
        "outputId": "98f09b95-86a5-4024-b4bc-f767f9f48407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.59.9\n",
            "    Uninstalling openai-1.59.9:\n",
            "      Successfully uninstalled openai-1.59.9\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "height": 64,
        "id": "WYzzgvG6Xbj3"
      },
      "outputs": [],
      "source": [
        "#from helper_functions import get_llm_response\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import openai\n",
        "\n",
        "# Retrieve the OpenAI API key from Google Colab secrets\n",
        "openai.api_key = userdata.get('openai')\n",
        "\n",
        "def get_llm_response(prompt):\n",
        "    \"\"\"This function takes a prompt as input and queries OpenAI's GPT model for a response.\"\"\"\n",
        "    # Send the request to OpenAI API\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful but concise AI assistant.\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "    )\n",
        "    response = completion.choices[0].message.content\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "r81WnED4YVFL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOe_frBWXbj4"
      },
      "source": [
        "* Write a prompt to create a recipe using `get_llm_response`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oPH5-O5aVX9",
        "outputId": "d972e480-5a4d-44c0-9d51-ec8ef0b7e4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Create a short recipe that uses the following ingredients:\n",
            "    ['chicken', 'broccoli', 'rice']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "height": 268,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DYWzKrnUXbj4",
        "outputId": "51512eb8-78b3-40f0-808f-89d868cf651b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-399f6084f64a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get the response from the LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_llm_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Print the LLM response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9142b3306ebe>\u001b[0m in \u001b[0;36mget_llm_response\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"This function takes a prompt as input and queries OpenAI's GPT model for a response.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Send the request to OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     completion = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ],
      "source": [
        "# Write a list of ingredients\n",
        "ingredients = ['chicken', 'broccoli', 'rice']\n",
        "\n",
        "# Write the prompt\n",
        "prompt = f\"\"\"\n",
        "    Create a short recipe that uses the following ingredients:\n",
        "    {ingredients}\n",
        "\"\"\"\n",
        "\n",
        "# Get the response from the LLM\n",
        "response = get_llm_response(prompt)\n",
        "\n",
        "# Print the LLM response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDC0QprIXbj4"
      },
      "source": [
        "## Opening a text file and saving it as a string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRKlA1NGXbj5"
      },
      "source": [
        "You will load data that has already been created and is stored üìÅ for you in files.\n",
        "\n",
        "* Start by loading an email that Daniel sent recently. It is stored in a '.txt' file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "id": "5mfdbWeYXbj5"
      },
      "outputs": [],
      "source": [
        "f = open(\"email.txt\", \"r\")\n",
        "email = f.read()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTbmcW_6Xbj5"
      },
      "source": [
        "* Print what it is 'inside' the email ‚úâÔ∏è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFCorh4CXbj6",
        "outputId": "91017cbb-9e15-44d4-9ffc-df112a59badc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: My Incredible Journey Around the World!\n",
            "\n",
            "Hi Mohammad,\n",
            "\n",
            "I hope you are doing greatl! I just returned from an incredible trip around the world. I started in New York, enjoying a Broadway show and the skyline. In Rio de Janeiro, I soaked up the sun on Copacabana Beach and hiked to Christ the Redeemer. Cape Town amazed me with its natural beauty and safari adventures. Paris was magical with its charming streets and delicious food. Istanbul's Hagia Sophia and Bosphorus cruise were unforgettable. Tokyo dazzled with its neon lights and delicious sushi. Finally, Sydney's Opera House and Bondi Beach were the perfect end to my journey. Can't wait to share more stories and photos when we catch up!\n",
            "\n",
            "Best,\n",
            "\n",
            "Daniel\n"
          ]
        }
      ],
      "source": [
        "print(email)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QItfdcbXbj6"
      },
      "source": [
        "<p style=\"background-color:#F5C780; padding:15px\"> ü§ñ <b>Use the Chatbot</b>:\n",
        "    <br><br>\n",
        "    Explain this code line by line:\n",
        "    <br><br>f = open(\"email.txt\", \"r\")\n",
        "    <br>email = f.read()\n",
        "    <br>f.close()\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdwG4bw7Xbj6"
      },
      "source": [
        "<p style=\"background-color:#F5C780; padding:15px\"> ü§ñ <b>Use the Chatbot</b>:\n",
        "    <br><br>\n",
        "    What happens if I don't close a file?\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bevWWudGXbj6"
      },
      "source": [
        "## Using LLMs to extract bullet points from the email"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK7Rhym3Xbj6"
      },
      "source": [
        "* Create a prompt to extract bullet points from Daniel's email ‚úâÔ∏è."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 149,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pQ0IxpSXbj7",
        "outputId": "39ca9b6f-b294-4ccc-eca4-b332b6a662ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extract bullet points from the following email. \n",
            "Include the sender information. \n",
            "\n",
            "Email:\n",
            "Subject: My Incredible Journey Around the World!\n",
            "\n",
            "Hi Mohammad,\n",
            "\n",
            "I hope you are doing greatl! I just returned from an incredible trip around the world. I started in New York, enjoying a Broadway show and the skyline. In Rio de Janeiro, I soaked up the sun on Copacabana Beach and hiked to Christ the Redeemer. Cape Town amazed me with its natural beauty and safari adventures. Paris was magical with its charming streets and delicious food. Istanbul's Hagia Sophia and Bosphorus cruise were unforgettable. Tokyo dazzled with its neon lights and delicious sushi. Finally, Sydney's Opera House and Bondi Beach were the perfect end to my journey. Can't wait to share more stories and photos when we catch up!\n",
            "\n",
            "Best,\n",
            "\n",
            "Daniel\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Extract bullet points from the following email.\n",
        "Include the sender information.\n",
        "\n",
        "Email:\n",
        "{email}\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_prgwqJbXbj7"
      },
      "source": [
        "* Run the ```get_llm_response``` function to get the response with bullet points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOCvlY2yXbj7",
        "outputId": "9524c2a9-222a-4eeb-9720-aef042f4caa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sender: Daniel\n",
            "\n",
            "- Started trip in New York\n",
            "- Visited Rio de Janeiro\n",
            "- Explored Cape Town\n",
            "- Enjoyed Paris\n",
            "- Explored Istanbul\n",
            "- Visited Tokyo\n",
            "- Ended trip in Sydney\n"
          ]
        }
      ],
      "source": [
        "bullet_points = get_llm_response(prompt)\n",
        "print(bullet_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41KpnMTzXbj7"
      },
      "source": [
        "* Print the LLM response in Markdown format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "SNtwQ76QXbj7",
        "outputId": "1d8d7c03-34c0-4d13-d623-887d019768d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sender: Daniel\n\n- Started trip in New York\n- Visited Rio de Janeiro\n- Explored Cape Town\n- Enjoyed Paris\n- Explored Istanbul\n- Visited Tokyo\n- Ended trip in Sydney"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print in Markdown format\n",
        "display(Markdown(bullet_points))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7NuvzmYXbj7"
      },
      "source": [
        "## Extra practice\n",
        "\n",
        "Try the exercises below to get an LLM to carry out different tasks using the email text you read in from file:\n",
        "\n",
        "### Exercise 1\n",
        "\n",
        "Complete the code below to identify all the **countries** mentioned in the email."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 200,
        "id": "pVem7CHIXbj7"
      },
      "outputs": [],
      "source": [
        "# Complete the code below to identify all of the countries mentioned\n",
        "# in the email\n",
        "prompt = f\"\"\"WRITE YOUR PROMPT HERE\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\"\"\"\n",
        "\n",
        "countries = get_llm_response(prompt)\n",
        "print(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLjly2QzXbj7"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Write code below to list all of the activities that Daniel did on his trip. You'll need to create a prompt and use either `get_llm_response` or `print_llm_response`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "id": "vFtAYecJXbj7"
      },
      "outputs": [],
      "source": [
        "# Write code below to list all of the activities that Daniel did on\n",
        "# his trip. You'll need to create a prompt and use either\n",
        "# get_llm_response or print_llm_response\n",
        "# START YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}